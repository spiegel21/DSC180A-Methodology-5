---
layout: home
---

## Personal Information
- **Name**: Eduardo Spiegel
- **Email**: espiegel@ucsd.edu
- **Section**: A16
- **Mentor**: Yusu Wang, Gal Mishne

## Quarter 2 Project Proposal Prompts

**1. What is the most interesting topic covered in your domain this quarter?**

I found the topic Graph Transformers to be particularly fascinating because it leverages the Transformer architecture that was previously used for text, and leverage certain properties like attention and positional encoding for applyin it to the graphs domain.

**2. Describe a potential investigation you would like to pursue for your Quarter 2 Project.**

I would like to explore the neural collapse phenomenon. Neural collapse is a fascinating phenomenon observed in the training of deep neural networks, where the within-class variability of the features collapses to their class means, and these means become maximally distant from each other. In the context of graph neural networks (GNNs), investigating neural collapse could provide insights into how GNNs learn to represent graph-structured data. There are studies of this phenomenon on Graph Convoulitonal Networks, but we want to explore if it also happens on Graph Transformers.

**3. What is a potential change youâ€™d make to the approach taken in your current Quarter 1 Project?**

I would like to benchmark a new model that was recently developed and from the results seen in the paper, looks like it's outperforming all GNNs. Benchmarking this model will help me understand better its architecture and ensure reproducibility of the results from the paper.

**4. What other techniques would you be interested in using in your project?**

I am interested in exploring techniques like contrastive learning, which can enhance the representation learning of graph-structured data without a heavy dependence on labeled data. Additionally, I would like to investigate advanced optimization algorithms to boost the training efficiency and performance of graph neural networks. Also, apply a lot of math in order to recreate the proofs of neural collapse, applied to Graph Transformers.
